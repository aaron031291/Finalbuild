#!/usr/bin/env python3
"""
AI-Powered Resource Monitor & Autoscaler - Optimized Version
"""

import os
import logging
import json
import time
from datetime import datetime
from typing import Dict, List, Deque, Optional, Tuple
from collections import deque
from contextlib import contextmanager

import psutil
import numpy as np
from prometheus_client import start_http_server, Gauge
from sklearn.ensemble import IsolationForest
from fastapi import APIRouter
import httpx
from pydantic import BaseSettings, PositiveFloat, PositiveInt

# ======================
# Configuration (Type-safe)
# ======================
class Settings(BaseSettings):
    check_interval: PositiveInt = 60
    training_window: PositiveInt = 604800  # 1 week in seconds
    prometheus_port: PositiveInt = 8001
    api_timeout: PositiveFloat = 5.0
    
    class Config:
        env_prefix = 'MONITOR_'
        case_sensitive = False

settings = Settings()

# ======================
# Core Monitoring System
# ======================
class ResourceMonitor:
    METRIC_NAMES = ('cpu', 'ram', 'gpu', 'iops', 'latency')
    
    def __init__(self):
        self._setup_prometheus()
        self._init_anomaly_detector()
        self.historical_data: Deque[np.ndarray] = deque(maxlen=settings.training_window)
        self._gpu_available = self._detect_gpu()

    def _setup_prometheus(self):
        self.gauges = {
            name: Gauge(f'system_{name}', f'{name.upper()} utilization') 
            for name in self.METRIC_NAMES
        }
        start_http_server(settings.prometheus_port)

    def _init_anomaly_detector(self):
        self.anomaly_detector = IsolationForest(
            n_estimators=100,
            contamination=0.1,
            n_jobs=-1  # Use all cores
        )

    def collect_metrics(self) -> Dict[str, float]:
        """Collect all metrics with minimal system calls"""
        with self._timed_metric('latency'):
            cpu = psutil.cpu_percent(interval=0.1)
            mem = psutil.virtual_memory().percent
            iops = self._get_iops()
            gpu = self._get_gpu_usage() if self._gpu_available else 0.0
            latency = 0.2  # Placeholder for actual measurement

        metrics = {
            'cpu': cpu,
            'ram': mem,
            'gpu': gpu,
            'iops': iops,
            'latency': latency,
            'timestamp': datetime.utcnow().isoformat()
        }
        
        # Update Prometheus gauges
        for name, value in metrics.items():
            if name in self.gauges:
                self.gauges[name].set(value)
        
        return metrics

    def analyze_metrics(self, metrics: Dict) -> Dict:
        """Efficient anomaly detection with batch processing"""
        features = np.array([metrics[name] for name in self.METRIC_NAMES[:4]])
        self.historical_data.append(features)
        
        if len(self.historical_data) % 100 == 0:  # Batch training
            self.anomaly_detector.fit(np.array(self.historical_data))
        
        anomaly_score = self.anomaly_detector.decision_function([features])[0]
        return {
            'anomaly_score': float(anomaly_score),
            'recommendations': self._generate_recommendations(metrics),
            'timestamp': metrics['timestamp']
        }

    def _generate_recommendations(self, metrics: Dict) -> List[str]:
        """Vectorized threshold checking"""
        thresholds = {
            'cpu': 85,
            'ram': 80,
            'gpu': 90,
            'latency': 0.5
        }
        return [
            f'scale_{metric}' for metric, threshold in thresholds.items()
            if metrics.get(metric, 0) > threshold
        ]

    @contextmanager
    def _timed_metric(self, name: str):
        """Context manager for latency measurement"""
        start = time.monotonic()
        yield
        duration = time.monotonic() - start
        self.gauges[name].set(duration)

    def _get_iops(self) -> float:
        """Get IOPS with single disk call"""
        io = psutil.disk_io_counters()
        return io.read_count + io.write_count

    def _detect_gpu(self) -> bool:
        """Check GPU availability once"""
        try:
            import pynvml
            pynvml.nvmlInit()
            return True
        except ImportError:
            return False

    def _get_gpu_usage(self) -> float:
        """GPU memory usage with error caching"""
        if not self._gpu_available:
            return 0.0
            
        try:
            import pynvml
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            return (info.used / info.total) * 100
        except:
            self._gpu_available = False
            return 0.0

# ======================
# FastAPI Integration
# ======================
monitor_router = APIRouter()
resource_monitor = ResourceMonitor()
client = httpx.AsyncClient(timeout=settings.api_timeout)

@monitor_router.get("/metrics/status")
async def get_system_status():
    metrics = resource_monitor.collect_metrics()
    analysis = resource_monitor.analyze_metrics(metrics)
    return {"metrics": metrics, "analysis": analysis}

@monitor_router.post("/scale")
async def handle_scaling():
    """Async scaling endpoint with circuit breaker"""
    try:
        metrics = resource_monitor.collect_metrics()
        analysis = resource_monitor.analyze_metrics(metrics)
        
        if analysis['recommendations']:
            response = await client.post(
                "https://api.cloud-provider.com/scale",
                json={"recommendations": analysis['recommendations']}
            )
            response.raise_for_status()
            return {"status": "scaling_triggered"}
            
        return {"status": "no_action_needed"}
    except httpx.RequestError as e:
        logging.error(f"Scaling failed: {str(e)}")
        return {"status": "error", "message": str(e)}

# ======================
# Optimized Background Task
# ======================
class BackgroundMonitor:
    def __init__(self):
        self._running = False
        self._thread = None

    def start(self):
        if not self._running:
            self._running = True
            self._thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self._thread.start()

    def stop(self):
        self._running = False
        if self._thread:
            self._thread.join()

    def _monitor_loop(self):
        while self._running:
            try:
                metrics = resource_monitor.collect_metrics()
                analysis = resource_monitor.analyze_metrics(metrics)
                
                if analysis['recommendations']:
                    logging.info(f"Auto-scaling: {analysis['recommendations']}")
                    asyncio.run(handle_scaling())
                    
                time.sleep(settings.check_interval)
            except Exception as e:
                logging.error(f"Monitoring error: {str(e)}")
                time.sleep(min(60, settings.check_interval))

# ======================
# Initialization
# ======================
def init_monitoring(app):
    app.include_router(monitor_router, prefix="/monitor")
    BackgroundMonitor().start()

# ======================
# Usage Example
# ======================
if __name__ == "__main__":
    from fastapi import FastAPI
    import uvicorn
    
    app = FastAPI()
    init_monitoring(app)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)